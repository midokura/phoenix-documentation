{"archive":{"blogPosts":[{"id":"v1.5","metadata":{"permalink":"/foo/blog/v1.5","source":"@site/blog/2026-01-16-phoenix-v1.5.md","title":"Phoenix v1.5","description":"We are pleased to announce the v1.5 release of Phoenix.","date":"2026-01-16T00:00:00.000Z","tags":[{"inline":false,"label":"Phoenix","permalink":"/foo/blog/tags/phoenix","description":"Phoenix tag description"}],"readingTime":4.6,"hasTruncateMarker":true,"authors":[{"name":"Jim Kennedy","title":"Docs @ Midokura","url":"https://github.com/jimken-mido","page":{"permalink":"/foo/blog/authors/jimkennedy"},"socials":{"github":"https://github.com/jimken-mido"},"imageURL":"https://avatars.githubusercontent.com/u/79317884?v=4","key":"jimkennedy"}],"frontMatter":{"slug":"v1.5","title":"Phoenix v1.5","authors":["jimkennedy"],"tags":["phoenix"]},"unlisted":false,"nextItem":{"title":"Phoenix v1.4","permalink":"/foo/blog/v1.4"}},"content":"We are pleased to announce the v1.5 release of Phoenix.\n\n## Overview\n\nThe updated Operator Reference sheet and release notes are included with this message. They describe the revised steps, configuration details, and changes for provisioning and managing a Phoenix cluster under the new release.\n\nHave a great weekend!\n\n<!-- truncate -->\n\n## Features\n\n### Kubernetes Cluster Management Enhancements\n\n#### Download Cluster Configuration Files\n\n- Users can now download kubeconfig files directly from the UI for active\nKubernetes clusters\n- This enables immediate access to clusters using standard Kubernetes tools like\nkubectl\n- The download button appears in the cluster details view for ready clusters\n\n#### Improved Cluster Information Display\n\n- Enhanced cluster details page with more comprehensive information:\n    - Cluster Health Status: Visual indicators showing whether clusters are\nhealthy, unhealthy, or in an unknown state\n    - Detailed Cluster Labels: Displays Kubernetes version, container runtime,\nnetwork plugin, and cloud provider information\n    - Node Health Details: Expandable view showing health status for individual\ncluster nodes\n    - Updated Timestamps: Shows both creation and last update dates\n- Clusters table now shows master count instead of creation date for better\nat-a-glance information\n\n#### Configurable Cluster Settings\n\n- System administrators can now configure cluster templates and settings through\nconfiguration files\n- No longer requires code changes to adjust cluster creation parameters\n- More flexible deployment options for different environments\n\n### Security Improvements\n\nSSH Key Validation\n\n- The system now validates SSH public keys when users add them\n- Invalid or malformed keys are rejected with clear error messages\n- Prevents issues that could occur later when using keys for server access\n\n### Infrastructure Automation\n\n#### Enhanced Compute Infrastructure\n\n- Support for compute-only nodes, enabling more flexible infrastructure scaling\n- Better integration between IaaS Console and Kubernetes cluster management\n(Magnum)\n\n#### Improved Deployment Experience\n\n- Deployment scripts now work from any directory, making setup easier\n- More consistent inventory management across the platform\n\n### Bug Fixes\n\n#### Application Fixes\n\n- Fixed an issue with OpenStack client operations that could affect multi-tenant\nenvironments\n- Improved reliability of cluster and infrastructure operations\n\n#### Infrastructure Reliability Fixes\n\n- Memory Leak Fix: Resolved memory leak in OpenStack networking that required\ndaily restarts\n- Boot Configuration: Fixed GRUB update issues that could prevent servers from\nbooting correctly\n- Hostname Resolution: Fixed hostname resolution problems that were affecting\nvarious services\n- Virtual Machine Issues: Resolved KVM VM hostname resolution problems\n- System Configuration: Fixed formatting issues in system configuration files\n\n#### Configuration Fixes\n\n- Fixed image download and storage configuration issues\n- Corrected Magnum network driver configuration\n- Fixed CoreDNS Kubernetes external service configuration\n\n### Performance & Reliability\n\n- Faster application startup - Grafana integration no longer blocks the application from starting\n- More reliable cluster operations with improved timeout handling\n- Better error handling and retry logic for infrastructure operations\n\n## Operator reference\n\nThis is the reference sheet for Phoenix v1.5, an end-to-end solution to operate private,\nmulti-tenant AI factories. Operators will find below an overview of the materials, infrastructure,\nand other requirements, and an entry point to the procedure to provision and configure the\nsystem.\n\nPlease contact support@midokura.com for more information.\n\n### System requirements\n\nNote: documentation files referenced here are provided in a downloadable artefact included in\nthe environment setup section.\n\n- Before proceeding, operators are expected to ensure that the underlying infrastructure\nmeets the system requirements listed below.\n- Operating system requirements for the OpenStack control nodes are available in the\ndocumentation file ./service-operator/OS_REQUIREMENTS.md\n- Operators are expected to set up their hardware according to our official Blueprint,\nspecifically with regard to network configuration, port and interface assignment.\n    - Base Operating System for OSt controllers should be ubuntu-24.04\n- Storage. Operators are expected to provide a Ceph cluster, integrated in the\ninfrastructure as defined in the blueprint. See more details in the Environment setup.\n- Set up a new Google Application that will be used as an SSO provider for the IaaS\nservice. To follow this process, consult the ./service-operator/GOOGLE_SSO_SETUP.md file in the documentation bundle described below.\n- Set up credentials for the private registry at ghcr.io/midokura. We will provide you\nwith this token via secure means, and it will be required during the control plane\ninstallation process (more info ./service-operator/GHCR_AUTHENTICATION.md).\n\n## Overview\n\nThe sections below provide references to materials required to proceed with the provisioning\nprocess, which takes place from the Bastion node shown in the blueprint. On a high level, the\nprocess is based on:\n- An installer of the network fabric controller.\n- A bundle of Ansible playbooks that will install and configure all components in the control\nplane.\n\n## Environment setup\n\nTo install the Phoenix cluster, the Operator will work from the bastion node reflected in the\nblueprint. The materials below must be available in the node before proceeding with the\ninstallation.\n\n1. Create a new directory ./phoenix. This will serve to store artefacts and playbooks. All\ncommands and paths in this document are relative to this directory.\n1. Download and extract the Documentation bundle. We will refer to documentation files from different sections of this document.\n1. Download the Network controller installer ISO and the network fabric configuration\n    - hedgehog-installer.iso\n    - hedgehog-fabric-configuration.yaml\n\n## Provisioning procedure\n\n### Network fabric setup\n\n- To install the network fabric controller, follow the instructions in ./service-operator/NETWORK_CONTROL_NODE_SETUP.md\n\n### Control plane installation\n\n- Prepare the Ceph cluster by following the steps explained in the documentation file ./service-operator/CEPH_SETUP.md.\n- Download and extract Ansible playbooks.\n- Download inventory.example.yml as the base to input the configuration specific to your cluster.\n- Execute them following the instructions in ./service-operator/DEPLOYMENT.md\n\n### IaaS Console - Tenant and User configuration\n\nTo create additional admin users, register tenants and tenant users, please refer to the\ninstructions in ./service-operator/IAAS_CONSOLE_CONFIGURATION.md\n\n### Baremetal Installation\n\nTo install a baremetal node, please refer to the instructions in the documentation file:\n./service-operator/INSTALL_BAREMETAL_NODE.md"},{"id":"v1.4","metadata":{"permalink":"/foo/blog/v1.4","source":"@site/blog/2025-12-24-phoenix-v1.4.md","title":"Phoenix v1.4","description":"We are pleased to announce the v1.4 release of Phoenix.","date":"2025-12-24T00:00:00.000Z","tags":[{"inline":false,"label":"Phoenix","permalink":"/foo/blog/tags/phoenix","description":"Phoenix tag description"}],"readingTime":4.51,"hasTruncateMarker":true,"authors":[{"name":"Jim Kennedy","title":"Docs @ Midokura","url":"https://github.com/jimken-mido","page":{"permalink":"/foo/blog/authors/jimkennedy"},"socials":{"github":"https://github.com/jimken-mido"},"imageURL":"https://avatars.githubusercontent.com/u/79317884?v=4","key":"jimkennedy"}],"frontMatter":{"slug":"v1.4","title":"Phoenix v1.4","authors":["jimkennedy"],"tags":["phoenix"]},"unlisted":false,"prevItem":{"title":"Phoenix v1.5","permalink":"/foo/blog/v1.5"},"nextItem":{"title":"Phoenix v1.3","permalink":"/foo/blog/v1.3"}},"content":"We are pleased to announce the v1.4 release of Phoenix.\n\n## Overview\n\nThis release introduces significant infrastructure automation improvements, enhanced observability with Grafana integration, expanded HedgeHog network management capabilities, and robust Kubernetes cluster provisioning through Magnum. The release focuses on improving deployment reliability, monitoring capabilities, and network infrastructure automation.\n\n<!-- truncate -->\n\n## Features\n\n### Observability & Monitoring\n\n- Grafana Integration (iaas-api): Added native Grafana configuration to iaas-api for seamless monitoring dashboard connectivity\n- Enhanced Observability Deployment: Increased Helm chart deployment timeout for more reliable observability stack installations\n- Improved Monitoring Configuration: Added comprehensive Grafana variables to inventory management\n\n### Network Infrastructure Automation\n\n- HedgeHog VM Provisioning: Automated download, creation, and provisioning of HedgeHog VMs with ISO installer support and installation log monitoring\n- HedgeHog Manifest Deployment: Automated deployment of HedgeHog network manifests with VPC peering support and boot NIC MAC labeling for IaaS Console integration\n\n### Container Orchestration\n\n- Magnum Kubernetes Templates: Added automatic Kubernetes cluster template provisioning with Kubernetes v1.28.9, OpenStack integrations (Cinder CSI, Keystone, Octavia), and production-ready configurations\n- Enhanced Magnum Support: Enabled Magnum service in both development and QA environments with proper cluster user trust configuration\n\n### Image & Asset Management\n\n- Multi-format Compression Support: Added support for gz, bz2, and xz compressed images using community.general.decompress module\n- Improved Image Handling: Enhanced image download and decompression with better error handling for both compressed and uncompressed images\n- Fedora CoreOS 38 Support: Added Fedora CoreOS 38 image support with custom image properties\n\n### Development & Operations\n\n- Script Reorganization: Renamed main.sh to platform-setup.sh for clearer script identification and easier reference\n- Enhanced CLI Experience: Added Phoenix banner and welcome banner for improved user experience\n- Improved Error Reporting: Enhanced Makefile to print paths of missing playbooks for better debugging\n\n## Bug Fixes\n\n- Fixed OpenStack pod resource limits: Increased default PID limit for OpenStack pods to prevent neutron_dhcp_agent exhaustion\n- Fixed image processing: Improved handling of uncompressed images with different filenames and enhanced compressed file validation\n- Fixed Kolla Ansible dependencies: Updated to latest 2025.1 release version of python-ironicclient (5.10.1)\n- Fixed CI/CD workflows: Improved credential handling, proper yq installation, and enhanced workflow path management\n- Fixed inventory management: Corrected bastion inventory location and improved ansible configuration handling\n- Fixed VPC networking: Avoided overlapping subnets between operator and tenant VPCs\nFixed deployment scripts: Enhanced vault password management and environment variable handling\n\nThis release contains 95+ commits focused on infrastructure automation, observability enhancements, and deployment reliability improvements. The changes span multiple components including gpu-infrastructure, iaas-console integration, HedgeHog network management, and OpenStack service expansion.\n\nMerry Christmas! ðŸŽ…\n\n## Operator reference\n\nThis is the reference sheet for Phoenix v1.4, an end-to-end solution to operate private,\nmulti-tenant AI factories. Operators will find below an overview of the materials, infrastructure,\nand other requirements, and an entry point to the procedure to provision and configure the\nsystem.\n\nPlease contact support@midokura.com for more information.\n\n### System requirements\n\nNote: documentation files referenced here are provided in a downloadable artefact included in\nthe environment setup section.\n\n- Before proceeding, operators are expected to ensure that the underlying infrastructure\nmeets the system requirements listed below.\n- Operating system requirements for the OpenStack control nodes are available in the\ndocumentation file ./service-operator/OS_REQUIREMENTS.md\n- Operators are expected to set up their hardware according to our official Blueprint,\nspecifically with regard to network configuration, port and interface assignment.\n-- Base Operating System for OSt controllers should be ubuntu-24.04\n- Storage. Operators are expected to provide a Ceph cluster, integrated in the\ninfrastructure as defined in the blueprint. See more details in the Environment setup.\n- Set up a new Google Application that will be used as an SSO provider for the IaaS\nservice. To follow this process, consult the\n./service-operator/GOOGLE_SSO_SETUP.md file in the documentation bundle\ndescribed below.\n- Set up credentials for the private registry at ghcr.io/midokura. We will provide you\nwith this token via secure means, and it will be required during the control plane\ninstallation process (more info ./service-operator/GHCR_AUTHENTICATION.md).\n\n## Overview\n\nThe sections below provide references to materials required to proceed with the provisioning\nprocess, which takes place from the Bastion node shown in the blueprint. On a high level, the\nprocess is based on:\n- An installer of the network fabric controller.\n- A bundle of Ansible playbooks that will install and configure all components in the control\nplane.\n\n## Environment setup\n\nTo install the Phoenix cluster, the Operator will work from the bastion node reflected in the\nblueprint. The materials below must be available in the node before proceeding with the\ninstallation.\n\n1. Create a new directory ./phoenix. This will serve to store artefacts and playbooks. All\ncommands and paths in this document are relative to this directory.\n2. Download and extract the Documentation bundle. We will refer to documentation files\nfrom different sections of this document.\n3. Download the Network controller installer ISO and the network fabric configuration\na. hedgehog-installer.iso\nb. hedgehog-fabric-configuration.yaml\n\n## Provisioning procedure\n\n### Network fabric setup\n\n- To install the network fabric controller, follow the instructions in\n./service-operator/NETWORK_CONTROL_NODE_SETUP.md\n\n### Control plane installation\n\n- Prepare the Ceph cluster by following the steps explained in the documentation file\n./service-operator/CEPH_SETUP.md.\n- Download and extract Ansible playbooks.\n- Download inventory.example.yml as the base to input the configuration specific to your\ncluster.\n- Execute them following the instructions in ./service-operator/DEPLOYMENT.md\n\n#### IaaS Console - Tenant and User configuration\n\nTo create additional admin users, register tenants and tenant users, please refer to the\ninstructions in ./service-operator/IAAS_CONSOLE_CONFIGURATION.md\n\n### Baremetal Installation\n\nTo install a baremetal node, please refer to the instructions in the documentation file:\n./service-operator/INSTALL_BAREMETAL_NODE.md"},{"id":"v1.3","metadata":{"permalink":"/foo/blog/v1.3","source":"@site/blog/2025-12-12-phoenix-v1.3.md","title":"Phoenix v1.3","description":"Version 1.3 of Phoenix is now available. This is a small release that primarily includes bug fixes and minor improvements.","date":"2025-12-12T00:00:00.000Z","tags":[{"inline":false,"label":"Phoenix","permalink":"/foo/blog/tags/phoenix","description":"Phoenix tag description"}],"readingTime":3.6,"hasTruncateMarker":true,"authors":[{"name":"Jim Kennedy","title":"Docs @ Midokura","url":"https://github.com/jimken-mido","page":{"permalink":"/foo/blog/authors/jimkennedy"},"socials":{"github":"https://github.com/jimken-mido"},"imageURL":"https://avatars.githubusercontent.com/u/79317884?v=4","key":"jimkennedy"}],"frontMatter":{"slug":"v1.3","title":"Phoenix v1.3","authors":["jimkennedy"],"tags":["phoenix"]},"unlisted":false,"prevItem":{"title":"Phoenix v1.4","permalink":"/foo/blog/v1.4"},"nextItem":{"title":"Phoenix v1.2","permalink":"/foo/blog/v1.2"}},"content":"Version 1.3 of Phoenix is now available. This is a small release that primarily includes bug fixes and minor improvements.\n\nThe updated Operator Reference sheet is included with this message. It describes the revised steps and configuration details required for provisioning and managing a Phoenix cluster under the new release. The release notes are also attached for a complete overview of the changes.\n\nHave a great weekend!\n\n<!-- truncate -->\n\n## Overview\n\nThis release focuses on IaaS Console improvements, and infrastructure reliability fixes.\n\n## Features\n\n### Infrastructure & Operations\n\n- Deployment Scripts (gpu-infrastructure): Improved vault password management with\nbetter security handling (uses vault file if available, prompts otherwise)\n- Backup Improvements (iaas-console): Fixed backup CronJob configuration and restore\nscript file paths\n  \n## Bug Fixes\n\n- Added OpenStack user and project as environment variables for better configuration\nmanagement (gpu-infrastructure)\n- Fixed settings to use global project as fallback when no tenant scope is present\n(iaas-console)\n- Fixed VPN allocation pool to prevent overlapping with gateway or Hedgehog switch (IPs\nnow start at .10) (iaas-console)\n- Fixed Docker build to properly copy animation folder (iaas-console)\n- Removed readOnlyRootFilesystem constraint from backup CronJob (iaas-console)\n- Corrected backup file path in restore-database.sh script (iaas-console)\n- Fixed Prometheus role key value in QA inventory (gpu-infrastructure)\n- Fixed Prometheus secret source references (gpu-infrastructure)\n- Fixed cinder-backup keyring configuration in Ceph config generation (gpu-infrastructure)\n\n## Operator reference\n\nThis is the reference sheet for Phoenix v1.3, an end-to-end solution to operate private,\nmulti-tenant AI factories. Operators will find below an overview of the materials, infrastructure,\nand other requirements, and an entry point to the procedure to provision and configure the\nsystem.\n\nPlease contact support@midokura.com for more information.\n\n### System requirements\n\nNote: documentation files referenced here are provided in a downloadable artefact included in\nthe environment setup section.\n\n- Before proceeding, operators are expected to ensure that the underlying infrastructure\nmeets the system requirements listed below.\n- Operating system requirements for the OpenStack control nodes are available in the\ndocumentation file ./service-operator/OS_REQUIREMENTS.md\n- Operators are expected to set up their hardware according to our official Blueprint,\nspecifically with regard to network configuration, port and interface assignment.\n  - Base Operating System for OSt controllers should be ubuntu-24.04\n- Storage. Operators are expected to provide a Ceph cluster, integrated in the\ninfrastructure as defined in the blueprint. See more details in the Environment setup.\n- Set up a new Google Application that will be used as an SSO provider for the IaaS\nservice. To follow this process, consult the\n./service-operator/GOOGLE_SSO_SETUP.md file in the documentation bundle\ndescribed below.\n- Set up credentials for the private registry at ghcr.io/midokura. We will provide you\nwith this token via secure means, and it will be required during the control plane\ninstallation process (more info ./service-operator/GHCR_AUTHENTICATION.md).\n\n## Overview\n\nThe sections below provide references to materials required to proceed with the provisioning\nprocess, which takes place from the Bastion node shown in the blueprint. On a high level, the\nprocess is based on:\n  - An installer of the network fabric controller.\n  - A bundle of Ansible playbooks that will install and configure all components in the control\nplane.\n\n## Environment setup\n\nTo install the Phoenix cluster, the Operator will work from the bastion node reflected in the\nblueprint. The materials below must be available in the node before proceeding with the\ninstallation.\n\n1. Create a new directory ./phoenix. This will serve to store artefacts and playbooks. All\ncommands and paths in this document are relative to this directory.\n1. Download and extract the Documentation bundle. We will refer to documentation files\nfrom different sections of this document.\n1. Download the Network controller installer ISO and the network fabric configuration\n   - hedgehog-installer.iso\n   - hedgehog-fabric-configuration.yaml\n\n## Provisioning procedure\n\n### Network fabric setup\n\n- To install the network fabric controller, follow the instructions in\n./service-operator/NETWORK_CONTROL_NODE_SETUP.md\n\n### Control plane installation\n\n- Prepare the Ceph cluster by following the steps explained in the documentation file\n./service-operator/CEPH_SETUP.md.\n- Download and extract Ansible playbooks.\n- Download inventory.example.yml as the base to input the configuration specific to your\ncluster.\n- Execute them following the instructions in ./service-operator/DEPLOYMENT.md\n\n### IaaS Console - Tenant and User configuration\n\nTo create additional admin users, register tenants and tenant users, please refer to the\ninstructions in ./service-operator/IAAS_CONSOLE_CONFIGURATION.md\n\n### Baremetal Installation\n\nTo install a baremetal node, please refer to the instructions in the documentation file:\n./service-operator/INSTALL_BAREMETAL_NODE.md"},{"id":"v1.2","metadata":{"permalink":"/foo/blog/v1.2","source":"@site/blog/2025-12-05-phoenix-v1.2.md","title":"Phoenix v1.2","description":"Version 1.2 of Phoenix is now available.","date":"2025-12-05T00:00:00.000Z","tags":[{"inline":false,"label":"Phoenix","permalink":"/foo/blog/tags/phoenix","description":"Phoenix tag description"}],"readingTime":3.2,"hasTruncateMarker":true,"authors":[{"name":"Jim Kennedy","title":"Docs @ Midokura","url":"https://github.com/jimken-mido","page":{"permalink":"/foo/blog/authors/jimkennedy"},"socials":{"github":"https://github.com/jimken-mido"},"imageURL":"https://avatars.githubusercontent.com/u/79317884?v=4","key":"jimkennedy"}],"frontMatter":{"slug":"v1.2","title":"Phoenix v1.2","authors":["jimkennedy"],"tags":["phoenix"]},"unlisted":false,"prevItem":{"title":"Phoenix v1.3","permalink":"/foo/blog/v1.3"},"nextItem":{"title":"Phoenix v1.1","permalink":"/foo/blog/v1.1"}},"content":"Version 1.2 of Phoenix is now available.\n\nThe updated Operator Reference sheet is included with this message. It describes the revised steps and configuration details required for provisioning and managing a Phoenix cluster under the new release.\n\nThanks.\n\n<!-- truncate -->\n\n## Overview\n\nThis release focuses on IaaS Console enhancements and network topology improvements.\n\n## Features\n\n### OpenStack Services\n\n- Bare-metal Provisioning: Added baremetal flavor for Ironic bare-metal instances with custom resource specs\n- CoreDNS External Plugin: Enabled k8s_external plugin in CoreDNS for external DNS server integration\n\n### IaaS Console Enhancements\n\n- VPN Configuration: Added VPN configuration support with\nflavor_name and image_name fields for VPN server provisioning\n- Features Configuration: Added configurable features (baremetals,\nvms) for per-deployment control\n\n### Network & VPC\n\n- VPC Peering: Prepared peering between operator and tenant VPCs for\nstorage access with shared IPv4Namespace\n- Storage Access: Enabled frontend operator and tenant VPCs to access\nstorage via VPC peering\n\n## Operator reference\n\nThis is the reference sheet for Phoenix v1.2, an end-to-end solution to operate private,\nmulti-tenant AI factories. Operators will find below an overview of the materials, infrastructure,\nand other requirements, and an entry point to the procedure to provision and configure the\nsystem.\n\nPlease contact support@midokura.com for more information.\n\n### System requirements\n\nNote: documentation files referenced here are provided in a downloadable artefact included in\nthe environment setup section.\n\n- Before proceeding, operators are expected to ensure that the underlying infrastructure\nmeets the system requirements listed below.\n- Operating system requirements for the OpenStack control nodes are available in the\ndocumentation file ./service-operator/OS_REQUIREMENTS.md\n- Operators are expected to set up their hardware according to our official Blueprint,\nspecifically with regard to network configuration, port and interface assignment.\n    - Base Operating System for OSt controllers should be ubuntu-24.04\n- Storage. Operators are expected to provide a Ceph cluster, integrated in the\ninfrastructure as defined in the blueprint. See more details in the Environment setup.\n- Set up a new Google Application that will be used as an SSO provider for the IaaS\nservice. To follow this process, consult the\n./service-operator/GOOGLE_SSO_SETUP.md file in the documentation bundle\ndescribed below.\n- Set up credentials for the private registry at ghcr.io/midokura. We will provide you\nwith this token via secure means, and it will be required during the control plane\ninstallation process (more info ./service-operator/GHCR_AUTHENTICATION.md).\n\n## Overview\n\nThe sections below provide references to materials required to proceed with the provisioning\nprocess, which takes place from the Bastion node shown in the blueprint. On a high level, the\nprocess is based on:\n- An installer of the network fabric controller.\n- A bundle of Ansible playbooks that will install and configure all components in the control\nplane.\n\n## Environment setup\n\nTo install the Phoenix cluster, the Operator will work from the bastion node reflected in the\nblueprint. The materials below must be available in the node before proceeding with the\ninstallation.\n\n1. Create a new directory ./phoenix. This will serve to store artefacts and playbooks. All\ncommands and paths in this document are relative to this directory.\n2. Download and extract the Documentation bundle. We will refer to documentation files\nfrom different sections of this document.\n3. Download the Network controller installer ISO and the network fabric configuration\n   - hedgehog-installer.iso\n   - hedgehog-fabric-configuration.yaml\n\n## Provisioning procedure\n\n### Network fabric setup\n\n- To install the network fabric controller, follow the instructions in\n./service-operator/NETWORK_CONTROL_NODE_SETUP.md\n\n### Control plane installation\n\n- Prepare the Ceph cluster by following the steps explained in the documentation file\n./service-operator/CEPH_SETUP.md.\n- Download and extract Ansible playbooks.\n- Download inventory.example.yml as the base to input the configuration specific to your\ncluster.\n- Execute them following the instructions in ./service-operator/DEPLOYMENT.md\n- \n### IaaS Console - Tenant and User configuration\nTo create additional admin users, register tenants and tenant users, please refer to the\ninstructions in ./service-operator/IAAS_CONSOLE_CONFIGURATION.md\n\n### Baremetal Installation\n\nTo install a baremetal node, please refer to the instructions in the documentation file:\n./service-operator/INSTALL_BAREMETAL_NODE.md"},{"id":"v1.1","metadata":{"permalink":"/foo/blog/v1.1","source":"@site/blog/2025-11-28-phoenix-v1.1.md","title":"Phoenix v1.1","description":"Version 1.1 of Phoenix is now available.","date":"2025-11-28T00:00:00.000Z","tags":[{"inline":false,"label":"Phoenix","permalink":"/foo/blog/tags/phoenix","description":"Phoenix tag description"}],"readingTime":2.64,"hasTruncateMarker":true,"authors":[{"name":"Jim Kennedy","title":"Docs @ Midokura","url":"https://github.com/jimken-mido","page":{"permalink":"/foo/blog/authors/jimkennedy"},"socials":{"github":"https://github.com/jimken-mido"},"imageURL":"https://avatars.githubusercontent.com/u/79317884?v=4","key":"jimkennedy"}],"frontMatter":{"slug":"v1.1","title":"Phoenix v1.1","authors":["jimkennedy"],"tags":["phoenix"]},"unlisted":false,"prevItem":{"title":"Phoenix v1.2","permalink":"/foo/blog/v1.2"},"nextItem":{"title":"Phoenix v1.0","permalink":"/foo/blog/v1.0"}},"content":"Version 1.1 of Phoenix is now available.\n\nThe updated Operator Reference sheet is included with this message. It describes the revised steps and configuration details required for provisioning and managing a Phoenix cluster under the new release.\n\nThanks.\n\n<!-- truncate -->\n\n## Operator reference\n\nThis is the reference sheet for Phoenix v1.1, an end-to-end solution to operate private,\nmulti-tenant AI factories. Operators will find below an overview of the materials, infrastructure,\nand other requirements, and an entry point to the procedure to provision and configure the\nsystem.\n\nPlease contact support@midokura.com for more information.\n\n## System requirements\n\nNote: documentation files referenced here are provided in a downloadable artefact included in the environment setup section.\n- Before proceeding, operators are expected to ensure that the underlying infrastructure\nmeets the system requirements listed below.\n- Operating system requirements for the OpenStack control nodes are available in the\ndocumentation file ./service-operator/OS_REQUIREMENTS.md\n- Operators are expected to set up their hardware according to our official Blueprint,\nspecifically with regard to network configuration, port and interface assignment.\n  - Base Operating System for OSt controllers should be ubuntu-24.04\n- Storage. Operators are expected to provide a Ceph cluster, integrated in the\ninfrastructure as defined in the blueprint. See more details in the Environment setup.\n- Set up a new Google Application that will be used as an SSO provider for the IaaS\nservice. To follow this process, consult the\n./service-operator/GOOGLE_SSO_SETUP.md file in the documentation bundle\ndescribed below.\n- Set up credentials for the private registry at ghcr.io/midokura. We will provide you\nwith this token via secure means, and it will be required during the control plane\ninstallation process (more info ./service-operator/GHCR_AUTHENTICATION.md).\n\n## Overview\n\nThe sections below provide references to materials required to proceed with the provisioning\nprocess, which takes place from the Bastion node shown in the blueprint. On a high level, the\nprocess is based on:\n- An installer of the network fabric controller.\n- A bundle of Ansible playbooks that will install and configure all components in the control plane.\n\n## Environment setup\n\nTo install the Phoenix cluster, the Operator will work from the bastion node reflected in the blueprint. The materials below must be available in the node before proceeding with the\ninstallation.\n\n1. Create a new directory ./phoenix. This will serve to store artefacts and playbooks. All\ncommands and paths in this document are relative to this directory.\n1. Download and extract the Documentation bundle. We will refer to documentation files\nfrom different sections of this document.\n1. Download the Network controller installer ISO\n    - Hedgehog-installer.iso\n  \n## Provisioning procedure\n\n### Network fabric setup\n\n- To install the network fabric controller, follow the instructions in\n./service-operator/NETWORK_CONTROL_NODE_SETUP.md\nControl plane installation\n- Prepare the Ceph cluster by following the steps explained in the documentation file\n./service-operator/CEPH_SETUP.md.\n- Download and extract Ansible playbooks.\n- Download inventory.example.yml as the base to input the configuration specific to your\ncluster.\n- Execute them following instructions in ./service-operator/DEPLOYMENT.md\n\n### IaaS Console - Tenant and User configuration\n\nTo create additional admin users, register tenants and tenant users, please refer to the\ninstructions in ./service-operator/IAAS_CONSOLE_CONFIGURATION.md\n\n### Baremetal Installation\n\nTo install a baremetal node, please refer to the instructions in the documentation file:\n./service-operator/INSTALL_BAREMETAL_NODE.md"},{"id":"v1.0","metadata":{"permalink":"/foo/blog/v1.0","source":"@site/blog/2025-11-21-phoenix-v1.0.md","title":"Phoenix v1.0","description":"We are pleased to announce the v1.0 release of Phoenix, our IaaS product for AI workloads.","date":"2025-11-21T00:00:00.000Z","tags":[{"inline":false,"label":"Phoenix","permalink":"/foo/blog/tags/phoenix","description":"Phoenix tag description"}],"readingTime":2.85,"hasTruncateMarker":true,"authors":[{"name":"Jim Kennedy","title":"Docs @ Midokura","url":"https://github.com/jimken-mido","page":{"permalink":"/foo/blog/authors/jimkennedy"},"socials":{"github":"https://github.com/jimken-mido"},"imageURL":"https://avatars.githubusercontent.com/u/79317884?v=4","key":"jimkennedy"}],"frontMatter":{"slug":"v1.0","title":"Phoenix v1.0","authors":["jimkennedy"],"tags":["phoenix"]},"unlisted":false,"prevItem":{"title":"Phoenix v1.1","permalink":"/foo/blog/v1.1"}},"content":"We are pleased to announce the v1.0 release of Phoenix, our IaaS product for AI workloads.\n\nThe Operator Reference sheet is attached to this email. This is provided to operators as the entry point to the provisioning procedure leading to a fully operational Phoenix cluster.\n\nThis version is installed in our QA/Staging cluster.\n\nCheers.\n\n<!-- truncate -->\n\n## Operator reference\n\nThis is the reference sheet for Phoenix v1.0, an end-to-end solution to operate private,\nmulti-tenant AI factories. Operators will find below an overview of the materials, infrastructure, and other requirements, and an entry point to the procedure to provision and configure the system.\n\nPlease contact support@midokura.com for more information.\n\n## System requirements\n\nNote: documentation files referenced here are provided in a downloadable artefact included in the environment setup section.\n\n- Before proceeding, operators are expected to ensure that the underlying infrastructure\nmeets the system requirements listed below.\n- Operating system requirements for the OpenStack control nodes are available in the\ndocumentation file ./service-operator/OS_REQUIREMENTS.md\n- Operators are expected to set up their hardware according to our official Blueprint,\nspecifically with regard to network configuration, port and interface assignment.\n    - Base Operating System for OSt controllers should be ubuntu-24.04\n- Storage. Operators are expected to provide a Ceph cluster, integrated in the\ninfrastructure as defined in the blueprint. See more details in the Environment setup.\n- Set up a new Google Application that will be used as an SSO provider for the IaaS\nservice. To follow this process, consult the\n./service-operator/GOOGLE_SSO_SETUP.md file in the documentation bundle\ndescribed below.\n- Set up credentials for the private registry at ghcr.io/midokura. We will provide you\nwith this token via secure means, and it will be required during the control plane\ninstallation process (more info ./service-operator/GHCR_AUTHENTICATION.md).\n\n## Overview\n\nThe sections below provide references to materials required to proceed with the provisioning process, which takes place from the Bastion node shown in the blueprint. On a high level, the process is based on:\n- An installer of the network fabric controller.\n- A bundle of Ansible playbooks that will install and configure all components in the control plane.\n\n## Environment setup\n\nTo install the Phoenix cluster, the Operator will work from the bastion node reflected in the blueprint. The materials below must be available in the node before proceeding with the installation.\n\n1. Create a new directory ./phoenix. This will serve to store artefacts and playbooks. All\ncommands and paths in this document are relative to this directory.\n2. Download and extract the Documentation bundle. We will refer to documentation files\nfrom different sections of this document.\n3. Download tenant VM images using the authenticated URLs below.\n    - phoenix-vms-2025-08-05-aa5ebab-noble-server-cloudimg.img\n    - phoenix-vms-main-6adcbb6-wireguard-vpn.img\n4. Download the Network controller installer ISO\n    - Hedgehog-installer.iso\n\n## Provisioning procedure\n\n### Network fabric setup\n\n- To install the network fabric controller, follow the instructions in\n./service-operator/NETWORK_CONTROL_NODE_SETUP.md\n\n### Control plane installation\n\n- Prepare the Ceph cluster by following the steps explained in the documentation file\n./service-operator/CEPH_SETUP.md.\n- Download and extract Ansible playbooks.\n- Download inventory.example.yml as the base to input the configuration specific to your\ncluster.\n- Execute them following instructions in ./service-operator/DEPLOYMENT.md\n\n### IaaS Console - Tenant and User configuration\n\nTo create additional admin users, register tenants and tenant users, please refer to the\ninstructions in ./service-operator/IAAS_CONSOLE_CONFIGURATION.md\n\n### Baremetal Installation\n\nTo install a baremetal node, please refer to the instructions in the documentation file:\n./service-operator/INSTALL_BAREMETAL_NODE.md"}]}}